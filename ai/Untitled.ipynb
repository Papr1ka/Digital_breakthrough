{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b62254d-3e4f-4a0e-abcf-9bc71754c4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\fktrc\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from evaluate) (2.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: dill in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from evaluate) (0.20.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\fktrc\\appdata\\roaming\\python\\python310\\site-packages (from evaluate) (24.0)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fktrc\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\fktrc\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\fktrc\\appdata\\roaming\\python\\python310\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\fktrc\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fktrc\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.1 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 30.7/84.1 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 84.1/84.1 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: responses, evaluate\n",
      "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa13816027204023ac9b616a248ea289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/780 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd68502f1114b0f80fa4fc6bf60a5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.81M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de59f793f4048519170b1af0d85842a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9aa170259341f9bc46c0080cc6e208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/217 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "import nltk\n",
    "import evaluate\n",
    "from PIL import Image\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tuman/vit-rugpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c97ea-cad8-41b3-b9c3-8ddce56412e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"tuman/vit-rugpt2-image-captioning\", cache_dir='E:\\\\ggames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9c237a-8db9-4e0c-b076-d13de76b96e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:31:48.665441Z",
     "iopub.status.busy": "2024-04-13T19:31:48.664790Z",
     "iopub.status.idle": "2024-04-13T19:31:48.861731Z",
     "shell.execute_reply": "2024-04-13T19:31:48.860501Z",
     "shell.execute_reply.started": "2024-04-13T19:31:48.665383Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"tuman/vit-rugpt2-image-captioning\", cache_dir='E:\\\\ggames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6a8507-ecc2-4b3d-88fc-28aa172b7bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:31:48.865139Z",
     "iopub.status.busy": "2024-04-13T19:31:48.864544Z",
     "iopub.status.idle": "2024-04-13T19:31:48.971427Z",
     "shell.execute_reply": "2024-04-13T19:31:48.970311Z",
     "shell.execute_reply.started": "2024-04-13T19:31:48.865107Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "dt = pd.read_csv('train.csv', sep=';', encoding='utf-8')\n",
    "path_train = '/home/jupyter/datasphere/project/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67757416-a894-43d6-a400-5e2281ae7b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:31:48.973677Z",
     "iopub.status.busy": "2024-04-13T19:31:48.972769Z",
     "iopub.status.idle": "2024-04-13T19:31:49.003314Z",
     "shell.execute_reply": "2024-04-13T19:31:49.001792Z",
     "shell.execute_reply.started": "2024-04-13T19:31:48.973641Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt_non_nan = dt[dt['description'].notna()].reset_index()\n",
    "dt_non_nan['path'] = path_train + '/' + (dt_non_nan['object_id']).astype(str) + '/' + dt_non_nan['img_name']\n",
    "dt_non_nan = dt_non_nan.drop(columns=['index', 'name', 'group', 'img_name', 'object_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03fba582-a102-4a02-80de-e82d410a2325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:31:49.005659Z",
     "iopub.status.busy": "2024-04-13T19:31:49.004775Z",
     "iopub.status.idle": "2024-04-13T19:31:49.020751Z",
     "shell.execute_reply": "2024-04-13T19:31:49.019605Z",
     "shell.execute_reply.started": "2024-04-13T19:31:49.005620Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(dt_non_nan['path'].values, \n",
    "                                                    dt_non_nan['description'].values, \n",
    "                                                    test_size=0.1)\n",
    "test_x, valid_x, test_y, valid_y = train_test_split(test_x, \n",
    "                                                    test_y, \n",
    "                                                    test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d96736d1-efd2-44c1-bcfd-0e0fe0213073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:31:51.803725Z",
     "iopub.status.busy": "2024-04-13T19:31:51.802624Z",
     "iopub.status.idle": "2024-04-13T19:31:51.830320Z",
     "shell.execute_reply": "2024-04-13T19:31:51.828892Z",
     "shell.execute_reply.started": "2024-04-13T19:31:51.803685Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pixels(image_paths):\n",
    "  images = []\n",
    "  for image_path in image_paths:\n",
    "    i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "      i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    images.append(i_image)\n",
    "\n",
    "  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "  return pixel_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ba58f77-1cf3-46c1-a557-d1604199b5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:31:52.217999Z",
     "iopub.status.busy": "2024-04-13T19:31:52.217021Z",
     "iopub.status.idle": "2024-04-13T19:31:52.234926Z",
     "shell.execute_reply": "2024-04-13T19:31:52.233846Z",
     "shell.execute_reply.started": "2024-04-13T19:31:52.217963Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_label(texts, max_target_length):\n",
    "    return tokenizer(texts,\n",
    "                     return_tensors='pt',\n",
    "                     padding='max_length',\n",
    "                     max_length=max_target_length,\n",
    "                     truncation=True).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ff856b-8afc-4c36-8c71-b70bddc98082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:31:53.689914Z",
     "iopub.status.busy": "2024-04-13T19:31:53.688900Z",
     "iopub.status.idle": "2024-04-13T19:31:53.705465Z",
     "shell.execute_reply": "2024-04-13T19:31:53.704433Z",
     "shell.execute_reply.started": "2024-04-13T19:31:53.689873Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class Custom(Dataset):\n",
    "    def __init__(self, X, y, max_target_length=128):\n",
    "        self.max_target_length = max_target_length\n",
    "        self.all_files = X\n",
    "        self.all_texts = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pixel_values = get_pixels([self.all_files[idx]])\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = get_label(self.all_texts[idx], self.max_target_length)[0]\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ffc2fe-5248-477f-a422-4b488a036887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:31:54.133246Z",
     "iopub.status.busy": "2024-04-13T19:31:54.132037Z",
     "iopub.status.idle": "2024-04-13T19:31:54.152172Z",
     "shell.execute_reply": "2024-04-13T19:31:54.150865Z",
     "shell.execute_reply.started": "2024-04-13T19:31:54.133167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Custom(train_x, train_y)\n",
    "test_dataset = Custom(test_x, test_y)\n",
    "valid_dataset = Custom(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f97b6e5-a1d1-462a-8b33-ce80319057b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:31:55.546023Z",
     "iopub.status.busy": "2024-04-13T19:31:55.544997Z",
     "iopub.status.idle": "2024-04-13T19:31:55.559266Z",
     "shell.execute_reply": "2024-04-13T19:31:55.558181Z",
     "shell.execute_reply.started": "2024-04-13T19:31:55.545984Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred, language='russian')) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label, language='russian')) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7f21c48-0e88-423e-a7ee-83ddee06874c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:31:55.986700Z",
     "iopub.status.busy": "2024-04-13T19:31:55.985745Z",
     "iopub.status.idle": "2024-04-13T19:31:57.949179Z",
     "shell.execute_reply": "2024-04-13T19:31:57.948124Z",
     "shell.execute_reply.started": "2024-04-13T19:31:55.986633Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 2.68MB/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1f8f28f-ab95-4691-9c13-b15ffa152286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:31:57.951983Z",
     "iopub.status.busy": "2024-04-13T19:31:57.951061Z",
     "iopub.status.idle": "2024-04-13T19:31:57.965182Z",
     "shell.execute_reply": "2024-04-13T19:31:57.963976Z",
     "shell.execute_reply.started": "2024-04-13T19:31:57.951946Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d30ecf86-e965-402b-a191-63b4fc5e533d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:32:01.271255Z",
     "iopub.status.busy": "2024-04-13T19:32:01.270308Z",
     "iopub.status.idle": "2024-04-13T19:32:03.252509Z",
     "shell.execute_reply": "2024-04-13T19:32:03.251364Z",
     "shell.execute_reply.started": "2024-04-13T19:32:01.271215Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    output_dir=\"./image-captioning-output\",\n",
    "    save_steps=3000,\n",
    "    report_to='clearml'\n",
    ")\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4806350-8131-4c34-9aec-6a80e5c91cea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:32:52.790833Z",
     "iopub.status.busy": "2024-04-13T19:32:52.789640Z",
     "iopub.status.idle": "2024-04-13T19:35:00.444412Z",
     "shell.execute_reply": "2024-04-13T19:35:00.441536Z",
     "shell.execute_reply.started": "2024-04-13T19:32:52.790781Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: overwriting (reusing) task id=da48a4eeeed04a57a55124ed1ea8ad7b\n",
      "2024-04-13 19:33:11,763 - clearml.Task - INFO - No repository found, storing script code instead\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.\n",
      "ClearML results page: https://app.clear.ml/projects/bed312fc3f7f41c9a552f47d265832db/experiments/da48a4eeeed04a57a55124ed1ea8ad7b/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported key of type '<class 'int'>' found when connecting dictionary. It will be converted to str\n",
      " 43%|████▎     | 3001/6942 [00:01<00:02, 1645.36it/s]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:01<00:07,  1.46it/s]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:02<00:09,  1.06it/s]\u001b[A\n",
      " 31%|███       | 4/13 [00:04<00:10,  1.14s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:05<00:10,  1.33s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:07<00:09,  1.35s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:08<00:08,  1.34s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:09<00:06,  1.35s/it]\u001b[A\n",
      " 43%|████▎     | 3001/6942 [00:17<00:02, 1645.36it/s]\n",
      " 77%|███████▋  | 10/13 [00:12<00:04,  1.41s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:14<00:02,  1.37s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:15<00:01,  1.36s/it]\u001b[A\n",
      "                                                     \n",
      " 43%|████▎     | 3001/6942 [00:22<00:02, 1645.36it/s]\n",
      "100%|██████████| 13/13 [00:17<00:00,  1.35s/it]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.816318929195404, 'eval_rouge1': 0.0373931623931624, 'eval_rouge2': 0.0, 'eval_rougeL': 0.038461538461538464, 'eval_rougeLsum': 0.038461538461538464, 'eval_runtime': 20.291, 'eval_samples_per_second': 2.563, 'eval_steps_per_second': 0.641, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3002/6942 [00:22<00:42, 93.73it/s]  \n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:01<00:07,  1.56it/s]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:02<00:09,  1.08it/s]\u001b[A\n",
      " 31%|███       | 4/13 [00:04<00:10,  1.12s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:05<00:10,  1.29s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:06<00:09,  1.30s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:08<00:07,  1.32s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:09<00:06,  1.32s/it]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:11<00:05,  1.34s/it]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:12<00:04,  1.39s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:14<00:03,  1.59s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:15<00:01,  1.50s/it]\u001b[A\n",
      "                                                   \n",
      " 43%|████▎     | 3002/6942 [00:41<00:42, 93.73it/s]\n",
      "100%|██████████| 13/13 [00:17<00:00,  1.45s/it]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8156265616416931, 'eval_rouge1': 0.0373931623931624, 'eval_rouge2': 0.0, 'eval_rougeL': 0.038461538461538464, 'eval_rougeLsum': 0.038461538461538464, 'eval_runtime': 18.9542, 'eval_samples_per_second': 2.743, 'eval_steps_per_second': 0.686, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3003/6942 [00:43<01:36, 40.92it/s]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:01<00:06,  1.59it/s]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:02<00:08,  1.11it/s]\u001b[A\n",
      " 31%|███       | 4/13 [00:03<00:09,  1.10s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:05<00:10,  1.27s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:06<00:09,  1.29s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:08<00:07,  1.30s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:09<00:06,  1.32s/it]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:10<00:05,  1.34s/it]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:12<00:04,  1.39s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:13<00:02,  1.37s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:15<00:01,  1.35s/it]\u001b[A\n",
      "                                                   \n",
      " 43%|████▎     | 3003/6942 [01:01<01:36, 40.92it/s]\n",
      "100%|██████████| 13/13 [00:16<00:00,  1.33s/it]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8149785399436951, 'eval_rouge1': 0.0373931623931624, 'eval_rouge2': 0.0, 'eval_rougeL': 0.038461538461538464, 'eval_rougeLsum': 0.038461538461538464, 'eval_runtime': 18.1427, 'eval_samples_per_second': 2.866, 'eval_steps_per_second': 0.717, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3004/6942 [01:02<02:48, 23.44it/s]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:01<00:07,  1.51it/s]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:02<00:09,  1.08it/s]\u001b[A\n",
      " 31%|███       | 4/13 [00:04<00:10,  1.12s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:05<00:10,  1.30s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:07<00:09,  1.31s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:08<00:07,  1.32s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:09<00:06,  1.32s/it]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:11<00:05,  1.34s/it]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:12<00:04,  1.37s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:13<00:02,  1.35s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:15<00:01,  1.33s/it]\u001b[A\n",
      "                                                   \n",
      " 43%|████▎     | 3004/6942 [01:20<02:48, 23.44it/s]\n",
      "100%|██████████| 13/13 [00:16<00:00,  1.34s/it]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8142771124839783, 'eval_rouge1': 0.0373931623931624, 'eval_rouge2': 0.0, 'eval_rougeL': 0.038461538461538464, 'eval_rougeLsum': 0.038461538461538464, 'eval_runtime': 18.2099, 'eval_samples_per_second': 2.856, 'eval_steps_per_second': 0.714, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3005/6942 [01:20<04:30, 14.55it/s]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:01<00:07,  1.49it/s]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:02<00:09,  1.04it/s]\u001b[A\n",
      " 31%|███       | 4/13 [00:04<00:10,  1.12s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:05<00:10,  1.28s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:06<00:08,  1.29s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:08<00:07,  1.31s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:09<00:06,  1.31s/it]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:10<00:05,  1.32s/it]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:12<00:04,  1.36s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:13<00:02,  1.36s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:15<00:01,  1.34s/it]\u001b[A\n",
      "                                                   \n",
      " 43%|████▎     | 3005/6942 [01:39<04:30, 14.55it/s]\n",
      "100%|██████████| 13/13 [00:16<00:00,  1.34s/it]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8137597441673279, 'eval_rouge1': 0.0373931623931624, 'eval_rouge2': 0.0, 'eval_rougeL': 0.038461538461538464, 'eval_rougeLsum': 0.038461538461538464, 'eval_runtime': 18.2538, 'eval_samples_per_second': 2.849, 'eval_steps_per_second': 0.712, 'epoch': 1.3}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported number of image dimensions: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-76cea7b55f9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         )\n\u001b[0;32m-> 1645\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1907\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1908\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrng_to_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2fc0f00b1b3e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# add labels (input_ids) by encoding the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_target_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-62e124001cd7>\u001b[0m in \u001b[0;36mget_pixels\u001b[0;34m(image_paths)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;34m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_rescale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_rescale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, image, size, resample, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"height\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"width\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         return resize(\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"height\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"width\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;31m# For all transformations, we want to keep the same data format as the input image unless otherwise specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m# The resized image from PIL will always have channels last, so find the input format first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_channel_dimension_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;31m# To maintain backwards compatibility with the resizing done in previous image feature extractors, we use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36minfer_channel_dimension_format\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mfirst_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unsupported number of image dimensions: {image.ndim}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_dim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported number of image dimensions: 0"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32595229-425f-4187-be90-fcbefda94d1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:32:49.906205Z",
     "iopub.status.busy": "2024-04-13T19:32:49.905130Z",
     "iopub.status.idle": "2024-04-13T19:32:49.927471Z",
     "shell.execute_reply": "2024-04-13T19:32:49.926234Z",
     "shell.execute_reply.started": "2024-04-13T19:32:49.906156Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10718a65-663f-4b08-ba03-baa10a37bfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
